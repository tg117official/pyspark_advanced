Step-by-Step Notes: Connecting PySpark to Redshift

#1. Set Up the Redshift Cluster
- Ensure the Redshift cluster is created and in the Available state in the AWS Management Console.
- Note down the following details:
  - Cluster Endpoint: e.g., `demo-cluster.cw38oxiq9pss.ap-southeast-2.redshift.amazonaws.com`
  - Port: Typically `5439`
  - Database Name: e.g., `dev`
  - User Credentials: Username and password (e.g., `awsuser` and `Admin_awsuser01`).

#2. Make the Redshift Cluster Publicly Accessible
- In the AWS Management Console:
  1. Navigate to the Redshift cluster's settings.
  2. Enable the Publicly Accessible option.
  3. Apply the changes.

#3. Update the Security Group for Redshift
- Allow incoming traffic on the Redshift port (`5439`):
  1. Go to the EC2 section of the AWS Management Console.
  2. Find the Security Group associated with your Redshift cluster.
  3. Add an Inbound Rule:
     - Type: Custom TCP Rule
     - Protocol: TCP
     - Port Range: 5439
     - Source:
       - Use `0.0.0.0/0` for public access (not recommended for production).
       - Use your specific IP address or range for restricted access.

#4. Download the Required JAR Files
- Download the Redshift JDBC driver (`redshift-jdbc42-2.1.0.31.jar`) from [Amazon's official site](https://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html).
- Ensure you have compatible Jackson library JARs:
  - `jackson-databind-2.15.0.jar`
  - `jackson-core-2.15.0.jar`
  - `jackson-annotations-2.15.0.jar`

#5. Configure the Spark Session
- Add the JARs to the Spark session using the `spark.jars` configuration:

from pyspark.sql import SparkSession

# Define the paths to the JARs
jars = r"C:\path\to\jackson-databind-2.15.0.jar," \
       r"C:\path\to\jackson-core-2.15.0.jar," \
       r"C:\path\to\jackson-annotations-2.15.0.jar," \
       r"C:\path\to\redshift-jdbc42-2.1.0.31.jar"

# Initialize Spark session
spark = SparkSession.builder \
    .appName("ReadFromRedshift") \
    .config("spark.jars", jars) \
    .getOrCreate()


#6. Configure the Redshift Connection
- Set up the connection details in your PySpark script:

redshift_url = "jdbc:redshift://demo-cluster.cw38oxiq9pss.ap-southeast-2.redshift.amazonaws.com:5439/dev"
redshift_properties = {
    "user": "awsuser",
    "password": "Admin_awsuser01",
    "driver": "com.amazon.redshift.jdbc42.Driver"
}


#7. Query the Redshift Table
- Use the Spark JDBC read method to query the table:

df = spark.read \
    .format("jdbc") \
    .option("url", redshift_url) \
    .option("dbtable", "public.sales") \
    .options(redshift_properties) \
    .load()

df.show()


#8. Verify Connectivity
- Run the script to ensure data is successfully retrieved from Redshift.



Key Notes
- Security: Be cautious when making the Redshift cluster publicly accessible. Restrict access to specific IPs or ranges wherever possible.
- Classpath: Ensure all JARs are correctly specified in the `spark.jars` configuration.
- Firewall: Ensure no local or network firewalls block the connection to the Redshift cluster.

