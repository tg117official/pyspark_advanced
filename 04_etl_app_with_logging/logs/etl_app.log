2024-12-06 16:08:27,791 - main_logger - INFO - SparkSession initialized successfully.
2024-12-06 16:08:33,413 - data_cleaning_logger - INFO - Data read from data/input/employee_new.csv successfully.
2024-12-06 16:08:33,452 - data_cleaning_logger - INFO - Data cleaned by dropping nulls and duplicates.
2024-12-06 16:08:35,852 - data_cleaning_logger - INFO - Clean data written to data/output/clean_data successfully.
2024-12-06 16:08:35,997 - transformations_logger - INFO - Clean data read from data/output/clean_data successfully.
2024-12-06 16:08:37,274 - transformations_logger - INFO - Transformation applied: Added 'total_salary' column.
2024-12-06 16:08:37,951 - main_logger - INFO - SparkSession stopped. ETL process completed.
2024-12-06 16:10:58,633 - main_logger - INFO - SparkSession initialized successfully.
2024-12-06 16:11:04,034 - data_cleaning_logger - INFO - Data read from data/input/employee_new.csv successfully.
2024-12-06 16:11:05,394 - data_cleaning_logger - INFO - Data cleaned by dropping nulls and duplicates.
2024-12-06 16:11:05,394 - data_cleaning_logger - ERROR - Error during data cleaning: 'NoneType' object has no attribute 'coalesce'
2024-12-06 16:11:05,395 - main_logger - CRITICAL - Critical error in ETL process: 'NoneType' object has no attribute 'coalesce'
2024-12-06 16:11:05,762 - main_logger - INFO - SparkSession stopped. ETL process completed.
2024-12-07 09:24:59,431 - main_logger - INFO - SparkSession initialized successfully.
2024-12-07 09:25:03,985 - data_cleaning_logger - INFO - Data read from data/input/employee_new.csv successfully.
2024-12-07 09:25:04,021 - data_cleaning_logger - INFO - Data cleaned by dropping nulls and duplicates.
2024-12-07 09:25:05,855 - data_cleaning_logger - INFO - Clean data written to data/output/clean_data successfully.
2024-12-07 09:25:05,999 - transformations_logger - INFO - Clean data read from data/output/clean_data successfully.
2024-12-07 09:25:07,073 - transformations_logger - INFO - Transformation applied: Added 'total_salary' column.
2024-12-07 09:25:07,290 - main_logger - INFO - SparkSession stopped. ETL process completed.
2024-12-10 09:59:55,125 - main_logger - INFO - SparkSession initialized successfully.
2024-12-10 09:59:55,127 - main_logger - CRITICAL - Critical error in ETL process: 'inp'
Traceback (most recent call last):
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\03_etl_app_with_logging\main.py", line 30, in main
    clean_data(spark, paths['inp'], paths['clean_data'])
KeyError: 'inp'
2024-12-10 09:59:55,288 - main_logger - INFO - SparkSession stopped. ETL process completed.
2024-12-12 15:39:37,565 - main_logger - INFO - SparkSession initialized successfully.
2024-12-12 15:39:46,231 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=808>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2024-12-12 15:39:46,235 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\Sandeep\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\pyspark\context.py", line 292, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\pyspark\context.py", line 1195, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\pyspark\sql\utils.py", line 111, in deco
    return f(*a, **kw)
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o13.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2024-12-12 15:39:46,238 - data_cleaning_logger - ERROR - Error during data cleaning: An error occurred while calling o29.csv
2024-12-12 15:39:46,239 - main_logger - CRITICAL - Critical error in ETL process: An error occurred while calling o29.csv
Traceback (most recent call last):
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\03_etl_app_with_logging\main.py", line 30, in main
    clean_data(spark, paths['input'], paths['clean_data'])
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\03_etl_app_with_logging\src\data_cleaning.py", line 13, in clean_data
    df = spark.read.csv(input_path, header=True, inferSchema=True)
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\pyspark\sql\readwriter.py", line 410, in csv
    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\pyspark\sql\utils.py", line 111, in deco
    return f(*a, **kw)
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o29.csv
2024-12-12 15:39:46,780 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\Sandeep\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Sandeep\PycharmProjects\pyspark_advanced\.venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
